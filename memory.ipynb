{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Memory\n",
    "\n",
    "\n",
    "대화형 AI 애플리케이션에서 가장 중요한 기능 중 하나는 이전 대화 내용을 기억하고 맥락을 유지하는 것입니다. LangChain은 다양한 메모리 관리 방식을 제공하여 상태를 가진(stateful) 대화형 애플리케이션을 구축할 수 있도록 도와줍니다."
   ],
   "id": "6fa14a85e2d763ad"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-30T07:54:06.728390Z",
     "start_time": "2025-08-30T07:53:52.750276Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_ollama import OllamaLLM\n",
    "\n",
    "llm = OllamaLLM(model=\"llama3.1\")\n",
    "\n",
    "response1 = llm.invoke(\"안녕하세요, 내 이름은 민수입니다.\")\n",
    "print(response1)\n",
    "\n",
    "response2 = llm.invoke(\"제 이름이 뭐였죠?\")\n",
    "print(response2)\n",
    "\n",
    "response3 = llm.invoke(\"바보야!!!\")\n",
    "print(response3)\n",
    "\n"
   ],
   "id": "4aa55efed787d684",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "안녕하세요! 반갑습니다. 민수님이라고 하면 어떤 분인지 궁금하네요. 뭐하고 계신가요?\n",
      "저는 이전에 우리의 대화를 기억하지 못합니다. 새로운 대화에서부터 시작할 수 있습니다.\n",
      "힘내세요!\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 메모리의 역할\n",
    "\n",
    "메모리는 대화형 애플리케이션에서 다음과 같은 중요한 역할을 합니다:\n",
    "\n",
    "1. **맥락 유지**: 이전 대화 내용을 기억하여 현재 대화에 반영합니다.\n",
    "2. **개인화**: 사용자에 대한 정보를 저장하여 더 개인화된 응답을 제공합니다.\n",
    "3. **상태 관리**: 대화의 상태를 추적하여 일관된 대화를 유지합니다.\n",
    "4. **향상된 사용자 경험**: 사용자가 반복해서 정보를 제공하지 않아도 되도록 하여 더 원활한 대화 경험을 제공합니다."
   ],
   "id": "e87ec7005ccc62a3"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-08-30T08:03:02.861500Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_core.chat_history import InMemoryChatMessageHistory\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"당신은 천문학 전문가입니다. 사용자와 친근한 대화를 나누며 천문학 질문에 답변해주세요.\"),\n",
    "    MessagesPlaceholder(variable_name=\"history\"),\n",
    "    (\"human\", \"{question}\")\n",
    "])\n",
    "\n",
    "store = {}\n",
    "\n",
    "llm = OllamaLLM(model=\"llama3.1\")\n",
    "\n",
    "chain = prompt | llm\n",
    "\n",
    "def get_session_history(session_id: str) -> InMemoryChatMessageHistory:\n",
    "    if session_id not in store:\n",
    "        store[session_id] = InMemoryChatMessageHistory()\n",
    "    return store[session_id]\n",
    "\n",
    "chain_with_history = RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"question\",\n",
    "    history_messages_key=\"history\"\n",
    ")\n",
    "\n",
    "config = {\"configurable\": {\"session_id\": \"astronomy_chat_1\"}}\n",
    "\n",
    "response1 = chain_with_history.invoke(\n",
    "    {\"question\": \"안녕하세요! 저는 지구과학을 공부하는 학생입니다.\"}, config=config\n",
    ")\n",
    "\n",
    "print(response1)\n",
    "\n",
    "response2 = chain_with_history.invoke(\n",
    "    {\"question\": \"태양계에서 가장 큰 행성은 무엇인가요?\"}, config=config\n",
    ")\n",
    "\n",
    "print(response2)\n",
    "\n",
    "response3 = chain_with_history.invoke(\n",
    "    {\"question\": \"그 행성의 위성에 대해 알려주세요.\"}, config=config\n",
    ")\n",
    "\n",
    "print(response3)\n",
    "\n",
    "config2 = {\"configurable\": {\"session_id\": \"astronomy_chat_2\"}}\n",
    "\n",
    "response4 = chain_with_history.invoke(\n",
    "    {\"question\": \"제가 누구인지 기억하나요?\"}, config=config2\n",
    ")\n",
    "\n",
    "print(response4)\n",
    "\n",
    "print(store)\n"
   ],
   "id": "72517fb8542b6708",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "안녕하세요! 네, 좋은 아침입니다! 지구과학을 공부하시다니 축하합니다! 천문학은 지구과학의 중요한 일부분이라고 생각합니다. 어떤 질문이 있으신가요? 별이나 행성에 대해 물어보세요!\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "1e2d9bfae908e03d"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
